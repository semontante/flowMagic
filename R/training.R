#' magicTrain_local
#' 
#' function to generate the local training models using the list of hierarchical training set. 
#' @param list_train_sets List of labeled dataframe to train generated by the get_local_train function.
#' @param n_tree Number of tree for random forest model.
#' @param train_model Type of training model. Default to random forest ("rf").
#' @param method_control Cross-validation method. Default to out-of-the-bag method (oob).
#' @param n_cores Number of cores to use. Default to 1.
#' @return List of models objects.
#' @export
#' @examples 
#' \donttest{magicTrain_local()}

magicTrain_hierarchy<-function(list_train_sets,n_tree=10,train_model="rf",method_control="oob",n_cores=1){
  set.seed(40)
  # list that contains the optimized model for each pop 
  # for each level (from top to bottom)
  list_models_sets_all_levels<-list()
  list_gated_training_set<-list()
  all_levels_name<-names(list_train_sets)
  start<-Sys.time()
  for(level_name in all_levels_name){
    level<-list_train_sets[[level_name]]
    list_all_models_current_level<-list() 
    # we take all the training set of the pops of this level
    # we select only the train dataframe excluding the label association
    ind_labels_slot<-grep("labels",names(level))
    label_association<-level[ind_labels_slot]
    level_pops<-level[-ind_labels_slot] # all training set of the current level
    for(train_set_pop in level_pops){
      ########## training current pop ############
      # get training model for the current local training set
      current_pop_under_training<-tail(colnames(train_set_pop),1)
      Xtrain_local_tot_original<-train_set_pop[,c(1,2)]
      # add column of label association
      ind<-grep(current_pop_under_training,names(label_association),fixed=T)
      label_association_current_pops<-label_association[ind]
      #train_set_pop<-add_labels_column(df = train_set_pop,labels_assocation = label_association_current_pops[[1]])
      # prepare data
      list_data_train_temp<-list()
      list_data_train_temp[[1]]<-train_set_pop
      ref_train<-get_train_data(paths_file = list_data_train_temp,n_cores = 8,prop_down = 0.90,normalize_data = F)
      #.....
      # execute training
      print(sprintf("####### current_child_pop_under_training:%s ########",current_pop_under_training))
      ref_model_info<-magicTrain(df_train = ref_train,n_cores = n_cores,train_model = train_model,
                                 method_control=method_control,type_y="classes")
      
      print("Training done")
      print("----- report the models and other info")
      # we keep track of the models
      list_all_models_current_level[[current_pop_under_training]][["ref_model_info"]]<-ref_model_info
      # we keep track also of the dimensions
      current_dims<-colnames(Xtrain_local_tot_original)
      list_all_models_current_level[[current_pop_under_training]][["Dimensions_set"]]<-current_dims
      # we keep track also of the label association
      #label_association_current_pops<-update_label_association(df = ref_model_info$ref_data)
      list_all_models_current_level[[current_pop_under_training]][["Labels_set"]]<-label_association_current_pops
    }
    
    list_models_sets_all_levels[[level_name]]<-list_all_models_current_level
  }
  end<-Sys.time()
  time_taken<-end-start
  print("training time:")
  print(time_taken)
  return(list(list_models_sets_all_levels=list_models_sets_all_levels))
}

#' magicTrain
#' 
#' function to generate one training model based on a list of training sets (no hierarchy). 
#' @param df_train training dataframe generated by the get_train_data function.
#' @param n_cores Number of cores to use. Default to 1.
#' @param train_model Type of training model. Default to rf.
#' @param k_cv  Number of k for cross-validation (if method control=cv)
#' @param list_index_train  List of vector of indices to use in training for each fold.
#' @param list_index_val  List of vector of indices to use as held out data for each fold.
#' @param n_tree  Number of trees for random forest.
#' @param tune_lenght Number of parameters values trained during cross-validation.
#' @param size_nnet_units Number of units in hidden layer (if train_model=nnet).
#' @param decay_nnet Decay parameter value for nnet model.
#' @param method_control Type of training control: oob or cv. Default to oob. 
#' @param type_y Type of response variable: classes (train to predict gates boundaries) or n_gates_info(train to predict number of gates).
#' @param seed_n Set seed. Default to 40.
#' @return model object.
#' @export
#' @examples 
#' \donttest{magicTrain()}


magicTrain<-function(df_train,n_cores=1,train_model="rf",k_cv=10,
                                     list_index_train=NULL,list_index_val=NULL,n_tree=10,tune_lenght=3,size_nnet_units=100,
                                     decay_nnet=0.1,method_control="oob",type_y="classes",seed_n=40){
  start<-Sys.time()
  # Get Xtrain, Ytrain and f_info
  print("prepare data")
  check_info<-colnames(df_train) %in% c("classes","plot_num","n_gates_info")
  inds_info<-which(check_info==T)
  df_info<-df_train[,inds_info]
  Xtrain<-df_train[,-inds_info]
  Ytrain<-as.character(df_info[,type_y]) # or n_gates_info
  # train model
  print("training...")
  cl <- makePSOCKcluster(n_cores)
  registerDoParallel(cl)
  set.seed(seed_n)
  if(train_model=="rf"){
    out_model<-magicTrain_rf(Xtrain = Xtrain,Ytrain = Ytrain,
                             list_index_train = list_index_train,list_index_val=list_index_val,n_tree = n_tree,
                             method_control=method_control,k_cv=k_cv)
  }else if(train_model=="dt"){
    out_model<-magicTrain_dt(Xtrain = Xtrain,Ytrain = Ytrain,k_cv = k_cv,
                             list_index_train = list_index_train,list_index_val=list_index_val,tune_lenght = tune_lenght)
    
  }else if(train_model=="nnet"){
    out_model<-magicTrain_nnet(Xtrain = Xtrain,Ytrain = Ytrain,k_cv = k_cv,
                               list_index_train=list_index_train,list_index_val=list_index_val,size = size_nnet_units,
                               decay = decay_nnet)
  }
  stopCluster(cl)
  end<-Sys.time()
  time_taken<-end-start
  print("Execution time:")
  print(time_taken)
  print("Done")
  return(out_model)
}


#' magicTrain_nnet
#' 
#' function to generate a neural net training model. 
#' @param Xtrain Dataframe of training features.
#' @param Ytrain Dataframe of labels (one column).
#' @param k_cv  Number of k for cross-validation.
#' @param list_index_train  List of vector of indices to use in training for each fold.
#' @param list_index_val  List of vector of indices to use as held out data for each fold.
#' @param size  Number of units in hidden layer (if train_model=nnet).
#' @param decay  Decay parameter value for nnet model.
#' @return model object.
#' @export
#' @examples 
#' \donttest{magicTrain_nnet()}


magicTrain_nnet<-function(Xtrain,Ytrain,k_cv=10,list_index_train=NULL,list_index_val=NULL,size=100,decay=0.1,tune_lenght=5){
  # size= number of units in the hidden layer (nnet uses only 1 hidden layer)
  nnGrid <-  expand.grid(size = size,
                         decay = decay)
  #----- train model
  print("--- train model")
  if(is.null(list_index_train)==T){
    model_nnet <- train(
      x=Xtrain, y=Ytrain, method = "nnet",
      tuneGrid = nnGrid,MaxNWts=9000,
      trControl = trainControl(method = "cv",number = k_cv,trim = T,returnData = F)
    )
  }else{
    model_nnet <- train(
      x=Xtrain, y=Ytrain, method = "nnet",MaxNWts=9000,
      trControl = trainControl(trim = T,returnData = F,index = list_index_train,indexOut = list_index_val),
      tuneLength = tune_lenght
    )
  }
  
  return(model_nnet)
}


#' magicTrain_rf
#' 
#' function to generate a random forest training model. 
#' @param Xtrain Dataframe of training features.
#' @param Ytrain Dataframe of labels (one column).
#' @param k_cv  Number of k for cross-validation (if method control=cv).
#' @param list_index_train  List of vector of indices to use in training for each fold.
#' @param list_index_val  List of vector of indices to use as held out data for each fold.
#' @param n_tree  Number of trees for random forest.
#' @param method_control Type of training control: oob or cv. Default to oob. 
#' @return model object.
#' @export
#' @examples 
#' \donttest{magicTrain_rf()}

magicTrain_rf<-function(Xtrain,Ytrain,list_index_train=NULL,list_index_val=NULL,n_tree=10,method_control="oob",k_cv=10){
  
  repGrid <- expand.grid(.mtry=ncol(Xtrain)) 
  #----- train model
  print("--- train model")
  if(is.null(list_index_train)==T){
    if(method_control=="oob"){
      model_rf <- train(
        x=Xtrain, y=Ytrain, method = "rf",metric = "Accuracy",ntree=n_tree,
        trControl = trainControl(method = "oob", trim = T,returnData = F),
        tuneGrid = repGrid
      )
    }else if(method_control=="cv"){
      model_rf <- train(
        x=Xtrain, y=Ytrain, method = "rf",metric = "Accuracy",ntree=n_tree,
        trControl = trainControl(method = "cv",number = k_cv,trim = T,returnData = F),
        tuneGrid = repGrid
      )
    }

  }else{
    model_rf <- train(
      x=Xtrain, y=Ytrain, method = "rf",metric = "Accuracy",ntree=n_tree,
      trControl = trainControl(trim = T,returnData = F,index = list_index_train,indexOut = list_index_val),
      tuneGrid = repGrid
    )
  }

  return(model_rf)
}


#' magicTrain_knn
#' 
#' function to generate a random forest training model. 
#' @param Xtrain Dataframe of training features.
#' @param Ytrain Dataframe of labels (one column).
#' @param k_cv  Number of k for cross-validation.
#' @param list_index_train  List of vector of indices to use in training for each fold.
#' @param list_index_val  List of vector of indices to use as held out data for each fold.
#' @param tune_lenght  Number of hyper parameters to test. Default to 5.
#' @return model object.
#' @export
#' @examples 
#' \donttest{magicTrain_knn()}


magicTrain_knn<-function(Xtrain,Ytrain,k_cv=10,list_index_train=NULL,list_index_val=NULL,tune_lenght=5){
  
  #----- train model
  if(is.null(list_index_train)==T){
    print("--- train model")
    model_nb <- train(
      x=Xtrain, y=Ytrain, method = "knn",
      trControl = trainControl(method = "cv",number = k_cv, trim = T)
    )
  }else{
    print("--- train model")
    model_nb <- train(
      x=Xtrain, y=Ytrain, method = "knn",
      trControl = trainControl(trim = T,returnData = F,index = list_index_train,indexOut = list_index_val),
      tuneLength = tune_lenght
    )
  }
  
  return(model_nb)
}

#' magicTrain_nb
#' 
#' function to generate a random forest training model. 
#' @param Xtrain Dataframe of training features.
#' @param Ytrain Dataframe of labels (one column).
#' @param k_cv  Number of k for cross-validation.
#' @param list_index_train  List of vector of indices to use in training for each fold.
#' @param list_index_val  List of vector of indices to use as held out data for each fold.
#' @param tune_lenght  Number of hyper parameters to test. Default to 5.
#' @return model object.
#' @export
#' @examples 
#' \donttest{magicTrain_nb()}

magicTrain_nb<-function(Xtrain,Ytrain,k_cv=10,list_index_train=NULL,list_index_val=NULL,tune_lenght=5){
  
  #----- train model
  if(is.null(list_index_train)==T){
    print("--- train model")
    model_nb <- train(
      x=Xtrain, y=Ytrain, method = "nb",
      trControl = trainControl(method = "cv",number = k_cv, trim = T)
    )
  }else{
    print("--- train model")
    model_nb <- train(
      x=Xtrain, y=Ytrain, method = "nb",
      trControl = trainControl(trim = T,returnData = F,index = list_index_train,indexOut = list_index_val),
      tuneLength = tune_lenght
    )
  }
  
  return(model_nb)
}

#' magicTrain_dt
#' 
#' function to generate a random forest training model. 
#' @param Xtrain Dataframe of training features.
#' @param Ytrain Dataframe of labels (one column).
#' @param k_cv  Number of k for cross-validation.
#' @param list_index_train  List of vector of indices to use in training for each fold.
#' @param list_index_val  List of vector of indices to use as held out data for each fold.
#' @param tune_lenght  Number of hyper parameters to test. Default to 5.
#' @return model object.
#' @export
#' @examples 
#' \donttest{magicTrain_dt()}

magicTrain_dt<-function(Xtrain,Ytrain,k_cv=10,list_index_train=NULL,list_index_val=NULL,tune_lenght=5){
  
  #----- train model
  print("--- train model")
  if(is.null(list_index_train)==T){
    model_dt <- train(
      x=Xtrain, y=Ytrain, method = "rpart",
      trControl = trainControl(method = "cv",number = k_cv, trim = T,returnData = F),
      tuneLength = tune_lenght,
      parms=list(split="information")
    )
  }else{
    model_dt <- train(
      x=Xtrain, y=Ytrain, method = "rpart",
      trControl = trainControl(trim = T,returnData = F,index = list_index_train,indexOut = list_index_val),
      tuneLength = tune_lenght,
      parms=list(split="information")
    )
  }
  
  return(model_dt)
}

